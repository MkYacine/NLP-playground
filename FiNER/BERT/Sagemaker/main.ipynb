{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e58521d-2323-4d19-8877-7647376e75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffda1b7-d504-4b05-a64a-4575a959b936",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf64933-9a67-4b92-bf8a-c97a619bbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session_bucket = \"finer-replication\"\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d2f7a-46a2-421d-aa90-bc57cb2ec8b7",
   "metadata": {},
   "source": [
    "## Uploading untokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17841d2b-138e-4212-9f7e-e5b94e8b502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_sentence(df):\n",
    "    \"\"\"Your existing grouping function\"\"\"\n",
    "    df['gold_token'] = df['gold_token'].fillna(\"None\")\n",
    "    grouped = df.groupby(['doc_idx', 'sent_idx']).agg({\n",
    "        'gold_token': list,\n",
    "        'gold_label': list\n",
    "    }).reset_index()\n",
    "    \n",
    "    return grouped[['gold_token', 'gold_label']].rename(columns={\n",
    "        'gold_token': 'words',\n",
    "        'gold_label': 'labels'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64d2a77-4d2e-49f8-91f9-fdfd51b7dce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa9afea87c24def83485ba942b2afae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23a116b15774a88be341bd74cb3f064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2356867f1149ccabf7bd88f37ab15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.csv:   0%|          | 0.00/135k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6b0ec3cd4e40969d4c46f97a74f9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/336k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b21dd9768a24bcdbb08c09fa4ef7125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/80531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878bf7d10c2443fd957f10a0fe83f06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60516aaedab42af81aaa377b0bfe7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"gtfintechlab/finer-ord\")\n",
    "\n",
    "# Convert splits to DataFrames and preprocess\n",
    "splits = ['train', 'validation', 'test']\n",
    "s3_locations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee689433-01c8-4c98-8ae4-624fb453a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(dataset[split])\n",
    "    \n",
    "    # Group sentences (your existing preprocessing)\n",
    "    processed_df = group_by_sentence(df)\n",
    "    \n",
    "    # Save locally first\n",
    "    local_path = f'/tmp/{split}.parquet'\n",
    "    processed_df.to_parquet(local_path)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_path = f's3://{sagemaker_session_bucket}/data/{split}'\n",
    "    aws_path = f'data/{split}'\n",
    "    \n",
    "    # Upload using boto3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(local_path, sagemaker_session_bucket, f'{aws_path}/data.parquet')\n",
    "    \n",
    "    # Store S3 location\n",
    "    s3_locations[split] = s3_path\n",
    "    \n",
    "    # Clean up local file\n",
    "    os.remove(local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7379908-2f3c-41c5-b617-fcb64b863e6c",
   "metadata": {},
   "source": [
    "## Uploading tokenized data with ProcessingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27100223-c275-4283-b949-54b495928f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "\n",
    "\n",
    "processor = HuggingFaceProcessor(\n",
    "    role=role,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name='ner-preprocessing',\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version='py39'\n",
    ")\n",
    "\n",
    "# Define input and output paths\n",
    "input_data = 's3://finer-replication/data'\n",
    "output_data = 's3://finer-replication/BERT-processed_data'\n",
    "\n",
    "# Run processing job\n",
    "processor.run(\n",
    "    code='preprocess.py',\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=input_data,\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='processed_data',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=output_data\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        '--input-data', '/opt/ml/processing/input',\n",
    "        '--output-data', '/opt/ml/processing/output',\n",
    "        '--model-id', 'bert-base-cased'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323ece0-a995-47ba-9486-845d5c571baa",
   "metadata": {},
   "source": [
    "## Uploading untokenized data directly from notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae2bbcf-1ac6-4047-b473-1e4a89a91113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a862d3e1-c991-41b5-85f9-768383da692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3262 training examples\n",
      "Loaded 402 validation examples\n",
      "Loaded 1075 test examples\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client and load data\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'finer-replication'\n",
    "\n",
    "# Load all splits\n",
    "train_df = pd.read_parquet(f's3://{bucket_name}/data/train/data.parquet')\n",
    "val_df = pd.read_parquet(f's3://{bucket_name}/data/validation/data.parquet')\n",
    "test_df = pd.read_parquet(f's3://{bucket_name}/data/test/data.parquet')\n",
    "\n",
    "print(f\"Loaded {len(train_df)} training examples\")\n",
    "print(f\"Loaded {len(val_df)} validation examples\")\n",
    "print(f\"Loaded {len(test_df)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3619ad0-f56c-488e-ba35-005b8435acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b7150c-868c-4d7f-91d3-ed7eccf31a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fff57c94564e0192ddbf5c8f59a621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train:   0%|          | 0/3262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d6c4d67b5942f3bc80dc2e05703889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation:   0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d31c7481349cc8a4d8999dff77450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test:   0%|          | 0/1075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def align_labels(labels, word_ids):\n",
    "    \"\"\"Align labels with tokenized input.\"\"\"\n",
    "    aligned_labels = []\n",
    "    last_word = None\n",
    "    begin2inside = {1: 2, 3: 4, 5: 6}  # B- to I- mapping\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != last_word:\n",
    "            aligned_labels.append(labels[word_idx])\n",
    "        else:\n",
    "            label = labels[word_idx]\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]\n",
    "            aligned_labels.append(label)\n",
    "        last_word = word_idx\n",
    "    \n",
    "    return aligned_labels\n",
    "\n",
    "def process_split(df, desc):\n",
    "    \"\"\"Process a dataframe of sentences and labels.\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        # Convert numpy arrays to lists\n",
    "        words = row['words'].tolist()\n",
    "        labels = row['labels'].tolist()\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        word_ids = tokenized.word_ids()\n",
    "        aligned_labels = align_labels(labels, word_ids)\n",
    "        \n",
    "        processed_data.append({\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask'],\n",
    "            'labels': aligned_labels\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Process all splits\n",
    "processed_train = process_split(train_df, \"Processing train\")\n",
    "processed_val = process_split(val_df, \"Processing validation\")\n",
    "processed_test = process_split(test_df, \"Processing test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd5d19ab-1ee0-485c-a8e1-11ae11df7a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data processed and saved to S3!\n"
     ]
    }
   ],
   "source": [
    "# Save processed data to temporary files first\n",
    "torch.save(processed_train, '/tmp/processed_train.pt')\n",
    "torch.save(processed_val, '/tmp/processed_val.pt')\n",
    "torch.save(processed_test, '/tmp/processed_test.pt')\n",
    "\n",
    "# Upload to S3\n",
    "s3.upload_file('/tmp/processed_train.pt', bucket_name, 'BERT-processed_data/train/processed_data.pt')\n",
    "s3.upload_file('/tmp/processed_val.pt', bucket_name, 'BERT-processed_data/validation/processed_data.pt')\n",
    "s3.upload_file('/tmp/processed_test.pt', bucket_name, 'BERT-processed_data/test/processed_data.pt')\n",
    "\n",
    "# Save and upload tokenizer\n",
    "tokenizer.save_pretrained('/tmp/tokenizer')\n",
    "for file in ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json']:\n",
    "    if os.path.exists(f'/tmp/tokenizer/{file}'):\n",
    "        s3.upload_file(f'/tmp/tokenizer/{file}', bucket_name, f'BERT-processed_data/{file}')\n",
    "\n",
    "print(\"All data processed and saved to S3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6e88c-962c-4693-b982-2e69961eb86e",
   "metadata": {},
   "source": [
    "## Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cf73520-9649-40be-8849-c5e8e5c5ca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-11-14-22-06-20-450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14 22:06:22 Starting - Starting the training job...\n",
      "2024-11-14 22:06:36 Starting - Preparing the instances for training...\n",
      "2024-11-14 22:07:04 Downloading - Downloading input data...\n",
      "2024-11-14 22:07:29 Downloading - Downloading the training image...............\n",
      "2024-11-14 22:10:27 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:38,800 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:38,819 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:38,833 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:38,836 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:39,975 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting seqeval>=1.2.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 2.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from seqeval>=1.2.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.9/site-packages (from seqeval>=1.2.2->-r requirements.txt (line 1)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval>=1.2.2->-r requirements.txt (line 1)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval>=1.2.2->-r requirements.txt (line 1)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval>=1.2.2->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=ad4815c780e95c69451623871d8e68287bcc3665369eb03917702403d7e2efd8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: seqeval\u001b[0m\n",
      "\u001b[34mSuccessfully installed seqeval-1.2.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 24.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,415 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,415 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,464 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,506 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,547 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,565 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"train_batch_size\": 8,\n",
      "        \"warmup_steps\": 500\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-11-14-22-06-20-450\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-971428921270/huggingface-pytorch-training-2024-11-14-22-06-20-450/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":8,\"learning_rate\":2e-05,\"train_batch_size\":8,\"warmup_steps\":500}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-971428921270/huggingface-pytorch-training-2024-11-14-22-06-20-450/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":8,\"learning_rate\":2e-05,\"train_batch_size\":8,\"warmup_steps\":500},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-11-14-22-06-20-450\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-971428921270/huggingface-pytorch-training-2024-11-14-22-06-20-450/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"8\",\"--learning_rate\",\"2e-05\",\"--train_batch_size\",\"8\",\"--warmup_steps\",\"500\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=500\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --eval_batch_size 8 --learning_rate 2e-05 --train_batch_size 8 --warmup_steps 500\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:43,605 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:46,290 - __main__ - INFO - Loaded train_dataset length: 3262\u001b[0m\n",
      "\u001b[34m2024-11-14 22:10:46,290 - __main__ - INFO - Loaded val_dataset length: 402\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 3262\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 3262\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 408\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 408\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 107725063\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 107725063\u001b[0m\n",
      "\u001b[34m0%|          | 0/408 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.051 algo-1:70 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.142 algo-1:70 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.143 algo-1:70 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.143 algo-1:70 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.144 algo-1:70 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-14 22:10:51.144 algo-1:70 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/408 [00:01<06:49,  1.01s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/408 [00:01<03:13,  2.09it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3/408 [00:01<02:08,  3.16it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/408 [00:01<01:36,  4.18it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6/408 [00:01<01:04,  6.22it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/408 [00:01<00:57,  6.93it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/408 [00:01<00:53,  7.43it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/408 [00:01<00:53,  7.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/408 [00:02<00:45,  8.64it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/408 [00:02<00:47,  8.27it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14/408 [00:02<00:43,  8.98it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 16/408 [00:02<00:41,  9.53it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17/408 [00:02<00:42,  9.29it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 19/408 [00:02<00:40,  9.63it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 20/408 [00:03<00:40,  9.48it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 21/408 [00:03<00:40,  9.56it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 23/408 [00:03<00:38,  9.91it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 24/408 [00:03<00:39,  9.82it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 25/408 [00:03<00:39,  9.78it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 26/408 [00:03<00:39,  9.76it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 27/408 [00:03<00:42,  8.88it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/408 [00:03<00:41,  9.08it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 29/408 [00:04<00:44,  8.51it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 30/408 [00:04<00:46,  8.12it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 31/408 [00:04<00:46,  8.18it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 32/408 [00:04<00:44,  8.40it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 34/408 [00:04<00:43,  8.57it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 35/408 [00:04<00:45,  8.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 36/408 [00:04<00:44,  8.43it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 37/408 [00:04<00:43,  8.62it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 39/408 [00:05<00:40,  9.00it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 40/408 [00:05<00:40,  9.16it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 41/408 [00:05<00:39,  9.27it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 42/408 [00:05<00:40,  9.00it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 44/408 [00:05<00:36,  9.85it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 45/408 [00:05<00:40,  9.04it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 46/408 [00:05<00:39,  9.22it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 48/408 [00:06<00:36,  9.86it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 50/408 [00:06<00:37,  9.56it/s]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 51/408 [00:06<00:37,  9.56it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 52/408 [00:06<00:39,  8.93it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 53/408 [00:06<00:40,  8.77it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 54/408 [00:06<00:40,  8.64it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 55/408 [00:06<00:41,  8.54it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 56/408 [00:07<00:41,  8.46it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 57/408 [00:07<00:40,  8.61it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 58/408 [00:07<00:39,  8.80it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 60/408 [00:07<00:42,  8.25it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 62/408 [00:07<00:38,  9.02it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 63/408 [00:07<00:39,  8.76it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 65/408 [00:08<00:36,  9.38it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 66/408 [00:08<00:37,  9.10it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 67/408 [00:08<00:37,  9.12it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 69/408 [00:08<00:34,  9.89it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 71/408 [00:08<00:33, 10.10it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 72/408 [00:08<00:34,  9.69it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 74/408 [00:08<00:33,  9.99it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 75/408 [00:09<00:33,  9.89it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 77/408 [00:09<00:32, 10.10it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 79/408 [00:09<00:31, 10.52it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 81/408 [00:09<00:30, 10.59it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 83/408 [00:09<00:29, 10.90it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 85/408 [00:09<00:32,  9.91it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 87/408 [00:10<00:32,  9.93it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 89/408 [00:10<00:32,  9.82it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 90/408 [00:10<00:32,  9.79it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 92/408 [00:10<00:32,  9.59it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 93/408 [00:10<00:33,  9.45it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 95/408 [00:11<00:31,  9.87it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 97/408 [00:11<00:31,  9.81it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 99/408 [00:11<00:30, 10.12it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 101/408 [00:11<00:30, 10.06it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 103/408 [00:11<00:30,  9.85it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 105/408 [00:12<00:31,  9.56it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 106/408 [00:12<00:31,  9.58it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 107/408 [00:12<00:33,  8.93it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 108/408 [00:12<00:33,  9.07it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 109/408 [00:12<00:33,  9.04it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 110/408 [00:12<00:34,  8.73it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 111/408 [00:12<00:33,  8.89it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 112/408 [00:12<00:32,  9.09it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 113/408 [00:12<00:33,  8.69it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 115/408 [00:13<00:31,  9.41it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 117/408 [00:13<00:30,  9.59it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 118/408 [00:13<00:33,  8.72it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 120/408 [00:13<00:30,  9.40it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 122/408 [00:13<00:31,  9.06it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 124/408 [00:14<00:30,  9.29it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 125/408 [00:14<00:32,  8.71it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 127/408 [00:14<00:29,  9.44it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 129/408 [00:14<00:28,  9.65it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 130/408 [00:14<00:28,  9.64it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 132/408 [00:14<00:28,  9.53it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 133/408 [00:15<00:28,  9.56it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 134/408 [00:15<00:28,  9.61it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 135/408 [00:15<00:28,  9.56it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 136/408 [00:15<00:28,  9.43it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 137/408 [00:15<00:28,  9.46it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 138/408 [00:15<00:29,  9.30it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 139/408 [00:15<00:30,  8.88it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 140/408 [00:15<00:34,  7.85it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 142/408 [00:16<00:29,  8.91it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 144/408 [00:16<00:28,  9.24it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 146/408 [00:16<00:27,  9.43it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 148/408 [00:16<00:27,  9.56it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 150/408 [00:16<00:26,  9.70it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 151/408 [00:17<00:30,  8.38it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 152/408 [00:17<00:29,  8.56it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 153/408 [00:17<00:30,  8.48it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 154/408 [00:17<00:32,  7.87it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 155/408 [00:17<00:30,  8.25it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 156/408 [00:17<00:30,  8.15it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 158/408 [00:17<00:30,  8.32it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 159/408 [00:18<00:28,  8.62it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 160/408 [00:18<00:27,  8.91it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 162/408 [00:18<00:26,  9.38it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 164/408 [00:18<00:25,  9.45it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 165/408 [00:18<00:25,  9.50it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 167/408 [00:18<00:24,  9.66it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 168/408 [00:19<00:27,  8.58it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 169/408 [00:19<00:27,  8.79it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 170/408 [00:19<00:27,  8.67it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 172/408 [00:19<00:25,  9.43it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 173/408 [00:19<00:25,  9.13it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 174/408 [00:19<00:25,  9.27it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 175/408 [00:19<00:25,  9.31it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 176/408 [00:19<00:25,  9.06it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 177/408 [00:20<00:28,  8.06it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 178/408 [00:20<00:27,  8.47it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 179/408 [00:20<00:30,  7.57it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 180/408 [00:20<00:28,  8.08it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 181/408 [00:20<00:28,  8.02it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 182/408 [00:20<00:28,  8.00it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 184/408 [00:20<00:25,  8.88it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 185/408 [00:20<00:24,  8.94it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 186/408 [00:21<00:24,  9.14it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 187/408 [00:21<00:24,  9.17it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 189/408 [00:21<00:21, 10.06it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 191/408 [00:21<00:22,  9.85it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 193/408 [00:21<00:21,  9.92it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 194/408 [00:21<00:22,  9.71it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 195/408 [00:22<00:27,  7.83it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 196/408 [00:22<00:26,  8.06it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 197/408 [00:22<00:25,  8.38it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 198/408 [00:22<00:24,  8.67it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 200/408 [00:22<00:22,  9.15it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 202/408 [00:22<00:21,  9.45it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 204/408 [00:23<00:21,  9.57it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 205/408 [00:23<00:21,  9.26it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 206/408 [00:23<00:22,  9.05it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 207/408 [00:23<00:24,  8.11it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 209/408 [00:23<00:22,  9.04it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 210/408 [00:23<00:21,  9.15it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 211/408 [00:23<00:22,  8.83it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 213/408 [00:24<00:20,  9.44it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 214/408 [00:24<00:21,  9.15it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 215/408 [00:24<00:21,  8.97it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 216/408 [00:24<00:21,  9.08it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 218/408 [00:24<00:20,  9.16it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 220/408 [00:24<00:19,  9.54it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 222/408 [00:25<00:19,  9.71it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 223/408 [00:25<00:19,  9.31it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 224/408 [00:25<00:19,  9.24it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 225/408 [00:25<00:20,  9.03it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 226/408 [00:25<00:20,  9.05it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 227/408 [00:25<00:19,  9.05it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 229/408 [00:25<00:18,  9.76it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 230/408 [00:25<00:18,  9.60it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 232/408 [00:26<00:18,  9.42it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 234/408 [00:26<00:17,  9.78it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 235/408 [00:26<00:18,  9.44it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 236/408 [00:26<00:18,  9.49it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 237/408 [00:26<00:18,  9.41it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 238/408 [00:26<00:18,  9.40it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 239/408 [00:26<00:17,  9.43it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 240/408 [00:26<00:18,  9.07it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 242/408 [00:27<00:17,  9.33it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 243/408 [00:27<00:18,  9.02it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 245/408 [00:27<00:16,  9.70it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 246/408 [00:27<00:16,  9.55it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 247/408 [00:27<00:17,  9.17it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 249/408 [00:27<00:16,  9.93it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 251/408 [00:28<00:16,  9.42it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 253/408 [00:28<00:17,  8.62it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 254/408 [00:28<00:17,  8.79it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 256/408 [00:28<00:16,  9.28it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 257/408 [00:28<00:16,  9.32it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 259/408 [00:28<00:15,  9.74it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 261/408 [00:29<00:14, 10.21it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 263/408 [00:29<00:14,  9.81it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 265/408 [00:29<00:14,  9.72it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 266/408 [00:29<00:14,  9.61it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 268/408 [00:29<00:13, 10.03it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 270/408 [00:30<00:14,  9.76it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 271/408 [00:30<00:15,  9.04it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 272/408 [00:30<00:15,  8.84it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 273/408 [00:30<00:15,  8.66it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 274/408 [00:30<00:15,  8.89it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 275/408 [00:30<00:15,  8.71it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 277/408 [00:30<00:13,  9.38it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 279/408 [00:31<00:13,  9.90it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 280/408 [00:31<00:13,  9.85it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 281/408 [00:31<00:12,  9.78it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 282/408 [00:31<00:12,  9.75it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 283/408 [00:31<00:13,  9.29it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 284/408 [00:31<00:13,  8.97it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 285/408 [00:31<00:13,  9.13it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 287/408 [00:31<00:12,  9.57it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 289/408 [00:32<00:12,  9.90it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 291/408 [00:32<00:11,  9.82it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 292/408 [00:32<00:11,  9.77it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 293/408 [00:32<00:11,  9.78it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 295/408 [00:32<00:11, 10.07it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 296/408 [00:32<00:11,  9.83it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 297/408 [00:32<00:11,  9.82it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 299/408 [00:33<00:11,  9.45it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 301/408 [00:33<00:10, 10.01it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 303/408 [00:33<00:10, 10.22it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 305/408 [00:33<00:09, 10.44it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 307/408 [00:33<00:09, 10.22it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 309/408 [00:34<00:09, 10.06it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 311/408 [00:34<00:09, 10.33it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 313/408 [00:34<00:09,  9.86it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 315/408 [00:34<00:09, 10.04it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 317/408 [00:34<00:09, 10.00it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 319/408 [00:35<00:09,  9.57it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 321/408 [00:35<00:08,  9.71it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 322/408 [00:35<00:08,  9.66it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 323/408 [00:35<00:08,  9.65it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 324/408 [00:35<00:08,  9.52it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 325/408 [00:35<00:08,  9.53it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 327/408 [00:35<00:08,  9.44it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 328/408 [00:36<00:08,  9.50it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 329/408 [00:36<00:08,  9.55it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 330/408 [00:36<00:08,  9.01it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 331/408 [00:36<00:08,  8.83it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 333/408 [00:36<00:08,  8.93it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 334/408 [00:36<00:08,  8.61it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 336/408 [00:36<00:07,  9.04it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 338/408 [00:37<00:07,  9.28it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 339/408 [00:37<00:07,  9.06it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 340/408 [00:37<00:07,  9.16it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 341/408 [00:37<00:07,  8.97it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 343/408 [00:37<00:06,  9.75it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 344/408 [00:37<00:06,  9.66it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 345/408 [00:38<00:08,  7.66it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 346/408 [00:38<00:07,  8.05it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 348/408 [00:38<00:06,  8.84it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 349/408 [00:38<00:06,  9.04it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 351/408 [00:38<00:06,  9.33it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 352/408 [00:38<00:06,  9.02it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 353/408 [00:38<00:05,  9.19it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 355/408 [00:39<00:05,  9.58it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 356/408 [00:39<00:05,  9.57it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 357/408 [00:39<00:05,  9.25it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 358/408 [00:39<00:05,  9.32it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 360/408 [00:39<00:05,  9.24it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 361/408 [00:39<00:05,  9.34it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 363/408 [00:39<00:04,  9.53it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 364/408 [00:40<00:04,  9.24it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 366/408 [00:40<00:04,  9.12it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 367/408 [00:40<00:04,  8.84it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 368/408 [00:40<00:04,  8.96it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 369/408 [00:40<00:04,  9.03it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 370/408 [00:40<00:04,  9.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 371/408 [00:40<00:03,  9.30it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 372/408 [00:40<00:03,  9.35it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 374/408 [00:41<00:03,  9.28it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 375/408 [00:41<00:03,  8.97it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 376/408 [00:41<00:03,  8.95it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 377/408 [00:41<00:03,  8.37it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 378/408 [00:41<00:03,  8.61it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 379/408 [00:41<00:03,  8.54it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 381/408 [00:41<00:02,  9.28it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 382/408 [00:42<00:02,  9.35it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 383/408 [00:42<00:02,  8.67it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 384/408 [00:42<00:03,  6.79it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 386/408 [00:42<00:02,  7.79it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 388/408 [00:42<00:02,  8.09it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 389/408 [00:42<00:02,  8.40it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 391/408 [00:43<00:01,  8.82it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 393/408 [00:43<00:01,  9.29it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 394/408 [00:43<00:01,  9.10it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 395/408 [00:43<00:01,  9.10it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 396/408 [00:43<00:01,  9.28it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 398/408 [00:43<00:01,  8.94it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 399/408 [00:44<00:01,  8.76it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 400/408 [00:44<00:00,  8.91it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4718, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 400/408 [00:44<00:00,  8.91it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 401/408 [00:44<00:00,  9.11it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 402/408 [00:44<00:00,  8.89it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 404/408 [00:44<00:00,  9.30it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 405/408 [00:44<00:00,  9.34it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 407/408 [00:44<00:00,  9.54it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 408/408 [00:44<00:00,  9.55it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 402\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 402\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/51 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/51 [00:00<00:01, 35.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/51 [00:00<00:01, 41.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 14/51 [00:00<00:00, 42.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19/51 [00:00<00:00, 42.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 24/51 [00:00<00:00, 39.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29/51 [00:00<00:00, 40.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 34/51 [00:00<00:00, 39.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 39/51 [00:00<00:00, 39.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 43/51 [00:01<00:00, 37.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 48/51 [00:01<00:00, 40.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10432113707065582, 'eval_f1': 0.8149377593360997, 'eval_runtime': 1.3872, 'eval_samples_per_second': 289.8, 'eval_steps_per_second': 36.766, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 408/408 [00:46<00:00,  9.55it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 51/51 [00:01<00:00, 40.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-408\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-408\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-408/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-408/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-408/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-408/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-408 (score: 0.8149377593360997).\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-408 (score: 0.8149377593360997).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 48.3371, 'train_samples_per_second': 67.484, 'train_steps_per_second': 8.441, 'train_loss': 0.4636666532971111, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 408/408 [00:48<00:00,  9.55it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 408/408 [00:48<00:00,  8.44it/s]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 402\u001b[0m\n",
      "\u001b[34mNum examples = 402\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/51 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/51 [00:00<00:01, 38.57it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/51 [00:00<00:00, 42.77it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 14/51 [00:00<00:00, 43.50it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 19/51 [00:00<00:00, 43.24it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 24/51 [00:00<00:00, 39.84it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 29/51 [00:00<00:00, 40.86it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 34/51 [00:00<00:00, 40.36it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 39/51 [00:00<00:00, 38.92it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 43/51 [00:01<00:00, 36.93it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 48/51 [00:01<00:00, 39.90it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 51/51 [00:01<00:00, 38.17it/s]\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - ***** Eval results *****\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - epoch = 1.0\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - eval_f1 = 0.8149377593360997\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - eval_loss = 0.10432113707065582\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - eval_runtime = 1.3761\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - eval_samples_per_second = 292.14\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:40,749 - __main__ - INFO - eval_steps_per_second = 37.063\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:41,850 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:41,850 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-14 22:11:41,851 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-14 22:11:47 Uploading - Uploading generated training model\n",
      "2024-11-14 22:12:30 Completed - Training job completed\n",
      "Training seconds: 326\n",
      "Billable seconds: 326\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 8,\n",
    "    'eval_batch_size': 8,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 500\n",
    "}\n",
    "\n",
    "# Create HuggingFace estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='train',\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version='py39',\n",
    "    hyperparameters=hyperparameters,\n",
    "    # use_spot_instances=True,  # Enable spot instances\n",
    "    # max_wait=7200,  # Maximum time to wait for spot instances (in seconds)\n",
    "    # max_run=3600,  # Maximum training time (in seconds)\n",
    ")\n",
    "\n",
    "# Start training\n",
    "huggingface_estimator.fit({\n",
    "    'training': 's3://finer-replication/BERT-processed_data'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076f958-6568-49bb-a415-f0a3f7c33204",
   "metadata": {},
   "source": [
    "## Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43fbed03-6c33-40ed-ae54-bd623742321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2024-11-14-22-13-09-947\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2024-11-14-22-13-09-947\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2024-11-14-22-13-09-947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "env = {\n",
    "    'HF_TASK': 'token-classification'\n",
    "}\n",
    "\n",
    "# Deploy the model with the specified environment variables\n",
    "predictor = huggingface_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f1fbfc3-b15e-469b-b142-aef7d3c34c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "   \"inputs\": \"Donald Trump is the United States president.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7fdf494-1246-4c00-95e9-101f3a205c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.8649447560310364,\n",
       "  'index': 1,\n",
       "  'word': 'Donald',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.7147530317306519,\n",
       "  'index': 2,\n",
       "  'word': 'Trump',\n",
       "  'start': 7,\n",
       "  'end': 12},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.7302929162979126,\n",
       "  'index': 5,\n",
       "  'word': 'United',\n",
       "  'start': 20,\n",
       "  'end': 26},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.7545739412307739,\n",
       "  'index': 6,\n",
       "  'word': 'States',\n",
       "  'start': 27,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6822941d-aa4f-45af-b1a8-8d4185b21c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-training-2024-11-14-21-58-08-584\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-training-2024-11-14-21-58-08-584\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
